{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":441417,"sourceType":"datasetVersion","datasetId":200079}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Importing the necessary libraries","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\nimport torch\nimport torch.nn.functional as F\nimport math,copy,re\nimport warnings\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport torchtext\nimport matplotlib.pyplot as plt\nfrom torchtext import data\nimport torch.optim as  optim \nimport os\nimport string\nfrom string import digits\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport re\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\nfrom keras.layers import Input, LSTM, Embedding, Dense\nfrom keras.models import Model\nif torch.cuda.is_available():  \n  dev = \"cuda:0\" \n\n  print(\"gpu up\")\nelse:  \n  dev = \"cpu\"  \ndevice = torch.device(dev)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-20T12:26:12.841902Z","iopub.execute_input":"2024-03-20T12:26:12.842220Z","iopub.status.idle":"2024-03-20T12:26:31.221989Z","shell.execute_reply.started":"2024-03-20T12:26:12.842188Z","shell.execute_reply":"2024-03-20T12:26:31.221052Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-03-20 12:26:21.744362: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-20 12:26:21.744460: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-20 12:26:21.865914: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"gpu up\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Upload and Read the Dataset","metadata":{}},{"cell_type":"code","source":"df=pd.read_csv(\"/kaggle/input/hindienglish-corpora/Hindi_English_Truncated_Corpus.csv\",encoding='utf-8')","metadata":{"execution":{"iopub.status.busy":"2024-03-20T12:26:31.224069Z","iopub.execute_input":"2024-03-20T12:26:31.224841Z","iopub.status.idle":"2024-03-20T12:26:32.075499Z","shell.execute_reply.started":"2024-03-20T12:26:31.224804Z","shell.execute_reply":"2024-03-20T12:26:32.074559Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Understanding the dataset features","metadata":{}},{"cell_type":"code","source":"df['source'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-03-20T12:26:32.076936Z","iopub.execute_input":"2024-03-20T12:26:32.077219Z","iopub.status.idle":"2024-03-20T12:26:32.110667Z","shell.execute_reply.started":"2024-03-20T12:26:32.077195Z","shell.execute_reply":"2024-03-20T12:26:32.109619Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"source\ntides        50000\nted          39881\nindic2012    37726\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"df=df[df['source']=='ted']","metadata":{"execution":{"iopub.status.busy":"2024-03-20T12:26:32.113563Z","iopub.execute_input":"2024-03-20T12:26:32.114277Z","iopub.status.idle":"2024-03-20T12:26:32.157135Z","shell.execute_reply.started":"2024-03-20T12:26:32.114240Z","shell.execute_reply":"2024-03-20T12:26:32.156254Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Applying the necessary data preprocessing ","metadata":{}},{"cell_type":"code","source":"pd.isnull(df).sum()","metadata":{"execution":{"iopub.status.busy":"2024-03-20T12:26:32.158223Z","iopub.execute_input":"2024-03-20T12:26:32.158481Z","iopub.status.idle":"2024-03-20T12:26:32.180191Z","shell.execute_reply.started":"2024-03-20T12:26:32.158459Z","shell.execute_reply":"2024-03-20T12:26:32.179096Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"source              0\nenglish_sentence    0\nhindi_sentence      0\ndtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"df=df[~pd.isnull(df['english_sentence'])]","metadata":{"execution":{"iopub.status.busy":"2024-03-20T12:26:32.181390Z","iopub.execute_input":"2024-03-20T12:26:32.181739Z","iopub.status.idle":"2024-03-20T12:26:32.193693Z","shell.execute_reply.started":"2024-03-20T12:26:32.181710Z","shell.execute_reply":"2024-03-20T12:26:32.192807Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"df.drop_duplicates(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-03-20T12:26:32.195109Z","iopub.execute_input":"2024-03-20T12:26:32.195533Z","iopub.status.idle":"2024-03-20T12:26:32.245907Z","shell.execute_reply.started":"2024-03-20T12:26:32.195487Z","shell.execute_reply":"2024-03-20T12:26:32.245065Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"df=df.sample(n=25000,random_state=42)\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2024-03-20T12:26:32.246859Z","iopub.execute_input":"2024-03-20T12:26:32.249664Z","iopub.status.idle":"2024-03-20T12:26:32.263829Z","shell.execute_reply.started":"2024-03-20T12:26:32.249638Z","shell.execute_reply":"2024-03-20T12:26:32.262888Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"(25000, 3)"},"metadata":{}}]},{"cell_type":"markdown","source":"# Text Preprocessing","metadata":{}},{"cell_type":"code","source":"# Lowercase all characters\ndf['english_sentence']=df['english_sentence'].apply(lambda x: x.lower())\ndf['hindi_sentence']=df['hindi_sentence'].apply(lambda x: x.lower())","metadata":{"execution":{"iopub.status.busy":"2024-03-20T12:26:32.265155Z","iopub.execute_input":"2024-03-20T12:26:32.265789Z","iopub.status.idle":"2024-03-20T12:26:32.308710Z","shell.execute_reply.started":"2024-03-20T12:26:32.265765Z","shell.execute_reply":"2024-03-20T12:26:32.307818Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Remove quotes\ndf['english_sentence']=df['english_sentence'].apply(lambda x: re.sub(\"'\", '', x))\ndf['hindi_sentence']=df['hindi_sentence'].apply(lambda x: re.sub(\"'\", '', x))","metadata":{"execution":{"iopub.status.busy":"2024-03-20T12:26:32.312007Z","iopub.execute_input":"2024-03-20T12:26:32.312331Z","iopub.status.idle":"2024-03-20T12:26:32.400057Z","shell.execute_reply.started":"2024-03-20T12:26:32.312308Z","shell.execute_reply":"2024-03-20T12:26:32.399179Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Remove all the special characters\nexclude = set(string.punctuation) \ndf['english_sentence']=df['english_sentence'].apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\ndf['hindi_sentence']=df['hindi_sentence'].apply(lambda x: ''.join(ch for ch in x if ch not in exclude))","metadata":{"execution":{"iopub.status.busy":"2024-03-20T12:26:32.401050Z","iopub.execute_input":"2024-03-20T12:26:32.401340Z","iopub.status.idle":"2024-03-20T12:26:32.775239Z","shell.execute_reply.started":"2024-03-20T12:26:32.401317Z","shell.execute_reply":"2024-03-20T12:26:32.774265Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Remove all numbers from text\nremove_digits = str.maketrans('', '', digits)\ndf['english_sentence']=df['english_sentence'].apply(lambda x: x.translate(remove_digits))\ndf['hindi_sentence']=df['hindi_sentence'].apply(lambda x: x.translate(remove_digits))\n\ndf['hindi_sentence'] = df['hindi_sentence'].apply(lambda x: re.sub(\"[२३०८१५७९४६]\", \"\", x))\n\n# Remove extra spaces\ndf['english_sentence']=df['english_sentence'].apply(lambda x: x.strip())\ndf['hindi_sentence']=df['hindi_sentence'].apply(lambda x: x.strip())\ndf['english_sentence']=df['english_sentence'].apply(lambda x: re.sub(\" +\", \" \", x))\ndf['hindi_sentence']=df['hindi_sentence'].apply(lambda x: re.sub(\" +\", \" \", x))","metadata":{"execution":{"iopub.status.busy":"2024-03-20T12:26:32.776467Z","iopub.execute_input":"2024-03-20T12:26:32.776781Z","iopub.status.idle":"2024-03-20T12:26:33.215585Z","shell.execute_reply.started":"2024-03-20T12:26:32.776755Z","shell.execute_reply":"2024-03-20T12:26:33.214637Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Add start and end tokens to target sequences\n# df['hindi_sentence'] = df['hindi_sentence'].apply(lambda x : 'START_ '+ x + ' _END')","metadata":{"execution":{"iopub.status.busy":"2024-03-20T12:26:33.216983Z","iopub.execute_input":"2024-03-20T12:26:33.217765Z","iopub.status.idle":"2024-03-20T12:26:33.221606Z","shell.execute_reply.started":"2024-03-20T12:26:33.217731Z","shell.execute_reply":"2024-03-20T12:26:33.220691Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"### Get English and Hindi Vocabulary\nall_eng_words=set()\nfor eng in df['english_sentence']:\n    for word in eng.split():\n        if word not in all_eng_words:\n            all_eng_words.add(word)\n\nall_hindi_words=set()\nfor hin in df['hindi_sentence']:\n    for word in hin.split():\n        if word not in all_hindi_words:\n            all_hindi_words.add(word)","metadata":{"execution":{"iopub.status.busy":"2024-03-20T12:26:33.223085Z","iopub.execute_input":"2024-03-20T12:26:33.223665Z","iopub.status.idle":"2024-03-20T12:26:33.352777Z","shell.execute_reply.started":"2024-03-20T12:26:33.223632Z","shell.execute_reply":"2024-03-20T12:26:33.351774Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"len(all_eng_words)","metadata":{"execution":{"iopub.status.busy":"2024-03-20T12:26:33.353902Z","iopub.execute_input":"2024-03-20T12:26:33.354176Z","iopub.status.idle":"2024-03-20T12:26:33.360248Z","shell.execute_reply.started":"2024-03-20T12:26:33.354152Z","shell.execute_reply":"2024-03-20T12:26:33.359218Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"14030"},"metadata":{}}]},{"cell_type":"code","source":"len(all_hindi_words)","metadata":{"execution":{"iopub.status.busy":"2024-03-20T12:26:33.361418Z","iopub.execute_input":"2024-03-20T12:26:33.361703Z","iopub.status.idle":"2024-03-20T12:26:33.371382Z","shell.execute_reply.started":"2024-03-20T12:26:33.361680Z","shell.execute_reply":"2024-03-20T12:26:33.370647Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"17538"},"metadata":{}}]},{"cell_type":"code","source":"df['length_eng_sentence']=df['english_sentence'].apply(lambda x:len(x.split(\" \")))\ndf['length_hin_sentence']=df['hindi_sentence'].apply(lambda x:len(x.split(\" \")))","metadata":{"execution":{"iopub.status.busy":"2024-03-20T12:26:33.372561Z","iopub.execute_input":"2024-03-20T12:26:33.372883Z","iopub.status.idle":"2024-03-20T12:26:33.444636Z","shell.execute_reply.started":"2024-03-20T12:26:33.372854Z","shell.execute_reply":"2024-03-20T12:26:33.443920Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"df[df['length_eng_sentence']>30].shape","metadata":{"execution":{"iopub.status.busy":"2024-03-20T12:26:33.445550Z","iopub.execute_input":"2024-03-20T12:26:33.445786Z","iopub.status.idle":"2024-03-20T12:26:33.456127Z","shell.execute_reply.started":"2024-03-20T12:26:33.445765Z","shell.execute_reply":"2024-03-20T12:26:33.455147Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"(0, 5)"},"metadata":{}}]},{"cell_type":"code","source":"df=df[df['length_eng_sentence']<=20]\ndf=df[df['length_hin_sentence']<=20]","metadata":{"execution":{"iopub.status.busy":"2024-03-20T12:26:33.457209Z","iopub.execute_input":"2024-03-20T12:26:33.457476Z","iopub.status.idle":"2024-03-20T12:26:33.476063Z","shell.execute_reply.started":"2024-03-20T12:26:33.457454Z","shell.execute_reply":"2024-03-20T12:26:33.475194Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2024-03-20T12:26:33.477149Z","iopub.execute_input":"2024-03-20T12:26:33.477401Z","iopub.status.idle":"2024-03-20T12:26:33.483479Z","shell.execute_reply.started":"2024-03-20T12:26:33.477380Z","shell.execute_reply":"2024-03-20T12:26:33.482569Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"(24916, 5)"},"metadata":{}}]},{"cell_type":"code","source":"print(\"maximum length of Hindi Sentence \",max(df['length_hin_sentence']))\nprint(\"maximum length of English Sentence \",max(df['length_eng_sentence']))","metadata":{"execution":{"iopub.status.busy":"2024-03-20T12:26:33.484637Z","iopub.execute_input":"2024-03-20T12:26:33.484916Z","iopub.status.idle":"2024-03-20T12:26:33.498176Z","shell.execute_reply.started":"2024-03-20T12:26:33.484894Z","shell.execute_reply":"2024-03-20T12:26:33.497194Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"maximum length of Hindi Sentence  20\nmaximum length of English Sentence  20\n","output_type":"stream"}]},{"cell_type":"code","source":"max_length_src=max(df['length_hin_sentence'])\nmax_length_tar=max(df['length_eng_sentence'])","metadata":{"execution":{"iopub.status.busy":"2024-03-20T12:26:33.499147Z","iopub.execute_input":"2024-03-20T12:26:33.499387Z","iopub.status.idle":"2024-03-20T12:26:33.515861Z","shell.execute_reply.started":"2024-03-20T12:26:33.499366Z","shell.execute_reply":"2024-03-20T12:26:33.515046Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"input_words = sorted(list(all_eng_words))\ntarget_words = sorted(list(all_hindi_words))\nnum_encoder_tokens = len(all_eng_words)\nnum_decoder_tokens = len(all_hindi_words)\nnum_encoder_tokens, num_decoder_tokens","metadata":{"execution":{"iopub.status.busy":"2024-03-20T12:26:33.516884Z","iopub.execute_input":"2024-03-20T12:26:33.517157Z","iopub.status.idle":"2024-03-20T12:26:33.544202Z","shell.execute_reply.started":"2024-03-20T12:26:33.517135Z","shell.execute_reply":"2024-03-20T12:26:33.543364Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"(14030, 17538)"},"metadata":{}}]},{"cell_type":"code","source":"num_decoder_tokens += 1 ","metadata":{"execution":{"iopub.status.busy":"2024-03-20T12:26:33.545230Z","iopub.execute_input":"2024-03-20T12:26:33.545483Z","iopub.status.idle":"2024-03-20T12:26:33.554061Z","shell.execute_reply.started":"2024-03-20T12:26:33.545461Z","shell.execute_reply":"2024-03-20T12:26:33.553209Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"input_token_index = dict([(word, i+1) for i, word in enumerate(input_words)])\ntarget_token_index = dict([(word, i+1) for i, word in enumerate(target_words)])","metadata":{"execution":{"iopub.status.busy":"2024-03-20T12:26:33.555153Z","iopub.execute_input":"2024-03-20T12:26:33.555429Z","iopub.status.idle":"2024-03-20T12:26:33.580233Z","shell.execute_reply.started":"2024-03-20T12:26:33.555407Z","shell.execute_reply":"2024-03-20T12:26:33.579553Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"reverse_input_char_index = dict((i, word) for word, i in input_token_index.items())\nreverse_target_char_index = dict((i, word) for word, i in target_token_index.items())","metadata":{"execution":{"iopub.status.busy":"2024-03-20T12:26:33.581122Z","iopub.execute_input":"2024-03-20T12:26:33.581389Z","iopub.status.idle":"2024-03-20T12:26:33.596014Z","shell.execute_reply.started":"2024-03-20T12:26:33.581351Z","shell.execute_reply":"2024-03-20T12:26:33.595225Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"df = shuffle(df)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-20T12:26:33.596912Z","iopub.execute_input":"2024-03-20T12:26:33.597155Z","iopub.status.idle":"2024-03-20T12:26:33.619958Z","shell.execute_reply.started":"2024-03-20T12:26:33.597134Z","shell.execute_reply":"2024-03-20T12:26:33.619110Z"},"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"       source                                   english_sentence  \\\n60826     ted                 not able to provide them with food   \n120362    ted  this is the list of types of thing that qualif...   \n17667     ted  is all about french soft power the british cou...   \n102120    ted                  when you could never climb a tree   \n68390     ted                and sound places us in time as well   \n\n                                           hindi_sentence  \\\n60826                            उन्हें खाना पूरा न दे कर   \n120362  ये उन चीज़ों की सूची है जो नुक्सान होने के काब...   \n17667   वह सिर्फ फ़्रांसिसी नर्म शक्ति और अंग्रेजी परि...   \n102120                 जब आप पेड़ पर कभी चढ़ ही नहीं सकते   \n68390                और आवाज़ हमे समय का ज्ञान भी देता है   \n\n        length_eng_sentence  length_hin_sentence  \n60826                     7                    6  \n120362                   12                   15  \n17667                     9                   12  \n102120                    7                    9  \n68390                     8                    9  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>source</th>\n      <th>english_sentence</th>\n      <th>hindi_sentence</th>\n      <th>length_eng_sentence</th>\n      <th>length_hin_sentence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>60826</th>\n      <td>ted</td>\n      <td>not able to provide them with food</td>\n      <td>उन्हें खाना पूरा न दे कर</td>\n      <td>7</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>120362</th>\n      <td>ted</td>\n      <td>this is the list of types of thing that qualif...</td>\n      <td>ये उन चीज़ों की सूची है जो नुक्सान होने के काब...</td>\n      <td>12</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>17667</th>\n      <td>ted</td>\n      <td>is all about french soft power the british cou...</td>\n      <td>वह सिर्फ फ़्रांसिसी नर्म शक्ति और अंग्रेजी परि...</td>\n      <td>9</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>102120</th>\n      <td>ted</td>\n      <td>when you could never climb a tree</td>\n      <td>जब आप पेड़ पर कभी चढ़ ही नहीं सकते</td>\n      <td>7</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>68390</th>\n      <td>ted</td>\n      <td>and sound places us in time as well</td>\n      <td>और आवाज़ हमे समय का ज्ञान भी देता है</td>\n      <td>8</td>\n      <td>9</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Split the data for testing & training","metadata":{}},{"cell_type":"code","source":"X, y = df['english_sentence'], df['hindi_sentence']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-03-20T12:26:33.625063Z","iopub.execute_input":"2024-03-20T12:26:33.625315Z","iopub.status.idle":"2024-03-20T12:26:33.633239Z","shell.execute_reply.started":"2024-03-20T12:26:33.625294Z","shell.execute_reply":"2024-03-20T12:26:33.632466Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"# Embedding Class:\n* Convert each word in the input sequence to an embedding vector. Embedding vectors will create a more semantic representation of each word.","metadata":{}},{"cell_type":"code","source":"class Embedding(nn.Module):\n    def __init__(self, vocab_size, embed_dim):\n        super(Embedding, self).__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim)\n    \n    def forward(self, x):\n        out = self.embed(x)\n        return out","metadata":{"execution":{"iopub.status.busy":"2024-03-20T12:26:33.634104Z","iopub.execute_input":"2024-03-20T12:26:33.634392Z","iopub.status.idle":"2024-03-20T12:26:33.641136Z","shell.execute_reply.started":"2024-03-20T12:26:33.634352Z","shell.execute_reply":"2024-03-20T12:26:33.640236Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"# PositionalEmbedding Class\n*  Generate positional encoding inorder for the model to make sense of the sentence, it needs to know what does the word means & its position in the sentence.","metadata":{}},{"cell_type":"code","source":"class PositionalEmbedding(nn.Module):\n    def __init__(self, max_seq_len, embed_model_dim):\n        super(PositionalEmbedding, self).__init__()\n        self.embed_dim = embed_model_dim\n        pe = torch.zeros(max_seq_len, self.embed_dim)\n        for pos in range(max_seq_len):\n            for i in range(0, self.embed_dim, 2):\n                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i) / self.embed_dim)))\n                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1)) / self.embed_dim)))\n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        x = x * math.sqrt(self.embed_dim)\n        seq_len = x.size(1)\n        x = x + torch.autograd.Variable(self.pe[:, :seq_len], requires_grad=False)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-03-20T12:26:33.642122Z","iopub.execute_input":"2024-03-20T12:26:33.642387Z","iopub.status.idle":"2024-03-20T12:26:33.652169Z","shell.execute_reply.started":"2024-03-20T12:26:33.642356Z","shell.execute_reply":"2024-03-20T12:26:33.651410Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"# MultiHeadAttention\n* We will have 8 self attention heads.\n\nSteps:\n\n1.  A key matrix,query matrix and a value matrix to generate key, query and value. These matrixes are learned during training.\n2. Calculate the score. Multiply query marix with key matrix [Q x K.t].\n3. Divide the output matrix with square root of dimension of key matrix and then apply Softmax over it.\n4. Then this gets multiply it with value matrix.\n5. Then we will pass this through a linear layer. This forms the output of multihead attention.","metadata":{}},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n    def __init__(self, embed_dim=512, n_heads=8):\n        super(MultiHeadAttention, self).__init__()\n        self.embed_dim = embed_dim\n        self.n_heads = n_heads\n        self.single_head_dim = int(self.embed_dim / self.n_heads)\n        self.query_matrix = nn.Linear(self.single_head_dim, self.single_head_dim, bias=False)\n        self.key_matrix = nn.Linear(self.single_head_dim, self.single_head_dim, bias=False)\n        self.value_matrix = nn.Linear(self.single_head_dim, self.single_head_dim, bias=False)\n        self.out = nn.Linear(self.n_heads * self.single_head_dim, self.embed_dim)\n\n    def forward(self, key, query, value, mask=None):\n        batch_size = key.size(0)\n        seq_length = key.size(1)\n        seq_length_query = query.size(1)\n        key = key.view(batch_size, seq_length, self.n_heads, self.single_head_dim)\n        query = query.view(batch_size, seq_length_query, self.n_heads, self.single_head_dim)\n        value = value.view(batch_size, seq_length, self.n_heads, self.single_head_dim)\n        \n        k = self.key_matrix(key)\n        q = self.query_matrix(query)\n        v = self.value_matrix(value)\n\n        q = q.transpose(1, 2)\n        k = k.transpose(1, 2)\n        v = v.transpose(1, 2)\n\n        k_adjusted = k.transpose(-1, -2)\n        product = torch.matmul(q, k_adjusted)\n\n        if mask is not None:\n            product = product.masked_fill(mask == 0, float(\"-1e20\"))\n\n        product = product / math.sqrt(self.single_head_dim)\n        scores = F.softmax(product, dim=-1)\n\n        scores = torch.matmul(scores, v)\n\n        concat = scores.transpose(1, 2).contiguous().view(batch_size, seq_length_query, self.single_head_dim * self.n_heads)\n        output = self.out(concat)\n\n        return output","metadata":{"execution":{"iopub.status.busy":"2024-03-20T12:26:33.653313Z","iopub.execute_input":"2024-03-20T12:26:33.653765Z","iopub.status.idle":"2024-03-20T12:26:33.666401Z","shell.execute_reply.started":"2024-03-20T12:26:33.653735Z","shell.execute_reply":"2024-03-20T12:26:33.665638Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"# Transformer Block\n* Processes the sequences by applying multi-head self-attention and feed-forward neural network layers with layer normalization and dropout, then transforming input sequences.","metadata":{}},{"cell_type":"code","source":"class TransformerBlock(nn.Module):\n    def __init__(self, embed_dim, expansion_factor=4, n_heads=8):\n        super(TransformerBlock, self).__init__()\n        self.attention = MultiHeadAttention(embed_dim, n_heads)\n        self.norm1 = nn.LayerNorm(embed_dim)\n        self.norm2 = nn.LayerNorm(embed_dim)\n        self.feed_forward = nn.Sequential(\n            nn.Linear(embed_dim, expansion_factor * embed_dim),\n            nn.ReLU(),\n            nn.Linear(expansion_factor * embed_dim, embed_dim)\n        )\n        self.dropout1 = nn.Dropout(0.2)\n        self.dropout2 = nn.Dropout(0.2)\n\n    def forward(self, key, query, value):\n        attention_out = self.attention(key, query, value)\n        attention_residual_out = attention_out + value\n        norm1_out = self.dropout1(self.norm1(attention_residual_out))\n\n        feed_fwd_out = self.feed_forward(norm1_out)\n        feed_fwd_residual_out = feed_fwd_out + norm1_out\n        norm2_out = self.dropout2(self.norm2(feed_fwd_residual_out))\n\n        return norm2_out","metadata":{"execution":{"iopub.status.busy":"2024-03-20T12:26:33.667532Z","iopub.execute_input":"2024-03-20T12:26:33.667921Z","iopub.status.idle":"2024-03-20T12:26:33.680651Z","shell.execute_reply.started":"2024-03-20T12:26:33.667892Z","shell.execute_reply":"2024-03-20T12:26:33.679798Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"# Transformer Encoder\n* Processes the input sequences by applying multiple Transformer blocks sequentially, embeddings & positional encodings to encode the input sequence into a transformed representation.","metadata":{}},{"cell_type":"code","source":"class TransformerEncoder(nn.Module):\n    def __init__(self, seq_len, vocab_size, embed_dim, num_layers=2, expansion_factor=4, n_heads=8):\n        super(TransformerEncoder, self).__init__()\n        self.embedding_layer = Embedding(vocab_size, embed_dim)\n        self.positional_encoder = PositionalEmbedding(seq_len, embed_dim)\n        self.layers = nn.ModuleList([TransformerBlock(embed_dim, expansion_factor, n_heads) for _ in range(num_layers)])\n    \n    def forward(self, x):\n        embed_out = self.embedding_layer(x)\n        out = self.positional_encoder(embed_out)\n        for layer in self.layers:\n            out = layer(out, out, out)\n\n        return out","metadata":{"execution":{"iopub.status.busy":"2024-03-20T12:26:33.681646Z","iopub.execute_input":"2024-03-20T12:26:33.681923Z","iopub.status.idle":"2024-03-20T12:26:33.695927Z","shell.execute_reply.started":"2024-03-20T12:26:33.681901Z","shell.execute_reply":"2024-03-20T12:26:33.695212Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"# Decoder Block","metadata":{}},{"cell_type":"code","source":"class DecoderBlock(nn.Module):\n    def __init__(self, embed_dim, expansion_factor=4, n_heads=8):\n        super(DecoderBlock, self).__init__()\n        self.attention = MultiHeadAttention(embed_dim, n_heads=8)\n        self.norm = nn.LayerNorm(embed_dim)\n        self.dropout = nn.Dropout(0.2)\n        self.transformer_block = TransformerBlock(embed_dim, expansion_factor, n_heads)\n    \n    def forward(self, key, query, x, mask):\n        attention = self.attention(x, x, x, mask=mask)\n        value = self.dropout(self.norm(attention + x))\n        out = self.transformer_block(key, query, value)\n\n        return out","metadata":{"execution":{"iopub.status.busy":"2024-03-20T12:26:33.696979Z","iopub.execute_input":"2024-03-20T12:26:33.697239Z","iopub.status.idle":"2024-03-20T12:26:33.708158Z","shell.execute_reply.started":"2024-03-20T12:26:33.697217Z","shell.execute_reply":"2024-03-20T12:26:33.707338Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"# Defining the TransformerDecoder class\nclass TransformerDecoder(nn.Module):\n    def __init__(self, target_vocab_size, embed_dim, seq_len, num_layers=2, expansion_factor=4, n_heads=8):\n        super(TransformerDecoder, self).__init__()\n        self.word_embedding = nn.Embedding(target_vocab_size, embed_dim)\n        self.position_embedding = PositionalEmbedding(seq_len, embed_dim)\n        self.layers = nn.ModuleList([DecoderBlock(embed_dim, expansion_factor=4, n_heads=8) for _ in range(num_layers)])\n        self.fc_out = nn.Linear(embed_dim, target_vocab_size)\n        self.dropout = nn.Dropout(0.2)\n\n    def forward(self, x, enc_out, mask):\n        x = self.word_embedding(x)\n        x = self.position_embedding(x)\n        x = self.dropout(x)\n     \n        for layer in self.layers:\n            x = layer(enc_out, x, enc_out, mask) \n\n        out = F.softmax(self.fc_out(x), dim=-1)\n\n\n        return out","metadata":{"execution":{"iopub.status.busy":"2024-03-20T12:26:33.709137Z","iopub.execute_input":"2024-03-20T12:26:33.709412Z","iopub.status.idle":"2024-03-20T12:26:33.722742Z","shell.execute_reply.started":"2024-03-20T12:26:33.709390Z","shell.execute_reply":"2024-03-20T12:26:33.721807Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"# Defining the Transformer class\nclass Transformer(nn.Module):\n    def __init__(self, embed_dim, src_vocab_size, target_vocab_size, seq_length, num_layers=2, expansion_factor=4, n_heads=8):\n        super(Transformer, self).__init__()\n        self.target_vocab_size = target_vocab_size\n        self.encoder = TransformerEncoder(seq_length, src_vocab_size, embed_dim, num_layers=num_layers, expansion_factor=expansion_factor, n_heads=n_heads)\n        self.decoder = TransformerDecoder(target_vocab_size, embed_dim, seq_length, num_layers=num_layers, expansion_factor=expansion_factor, n_heads=n_heads)\n    \n    def make_trg_mask(self, trg):\n        batch_size, trg_len = trg.shape\n        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(batch_size, 1, trg_len, trg_len)\n        return trg_mask\n    def decode(self, src, trg):\n        trg_mask = self.make_trg_mask(trg)\n        enc_out = self.encoder(src)\n        out_labels = []\n        batch_size, seq_len = src.shape[0], src.shape[1]\n        out = trg\n        for i in range(seq_len):\n            out = self.decoder(out, enc_out, trg_mask)\n            out = out[:, -1, :]\n            out = out.argmax(-1)\n            out_labels.append(out.item())\n            out = torch.unsqueeze(out, axis=0)\n        return out_labels\n    \n    def forward(self, src, trg):\n        trg_mask = self.make_trg_mask(trg)\n        enc_out = self.encoder(src)\n        outputs = self.decoder(trg, enc_out, trg_mask)\n        return outputs","metadata":{"execution":{"iopub.status.busy":"2024-03-20T12:26:33.723754Z","iopub.execute_input":"2024-03-20T12:26:33.723979Z","iopub.status.idle":"2024-03-20T12:26:33.734366Z","shell.execute_reply.started":"2024-03-20T12:26:33.723959Z","shell.execute_reply":"2024-03-20T12:26:33.733564Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"# Instantiating the Transformer model\nsrc_vocab_size = 11\ntarget_vocab_size = 11\nnum_layers = 6\nseq_length = 12\nmodel = Transformer(embed_dim=512, src_vocab_size=src_vocab_size, \n                    target_vocab_size=target_vocab_size, seq_length=seq_length,\n                    num_layers=num_layers, expansion_factor=4, n_heads=8)","metadata":{"execution":{"iopub.status.busy":"2024-03-20T12:26:33.735282Z","iopub.execute_input":"2024-03-20T12:26:33.735601Z","iopub.status.idle":"2024-03-20T12:26:34.204700Z","shell.execute_reply.started":"2024-03-20T12:26:33.735579Z","shell.execute_reply":"2024-03-20T12:26:34.203908Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"print(model)","metadata":{"execution":{"iopub.status.busy":"2024-03-20T12:26:34.205856Z","iopub.execute_input":"2024-03-20T12:26:34.206209Z","iopub.status.idle":"2024-03-20T12:26:34.213123Z","shell.execute_reply.started":"2024-03-20T12:26:34.206177Z","shell.execute_reply":"2024-03-20T12:26:34.212076Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"Transformer(\n  (encoder): TransformerEncoder(\n    (embedding_layer): Embedding(\n      (embed): Embedding(11, 512)\n    )\n    (positional_encoder): PositionalEmbedding()\n    (layers): ModuleList(\n      (0-5): 6 x TransformerBlock(\n        (attention): MultiHeadAttention(\n          (query_matrix): Linear(in_features=64, out_features=64, bias=False)\n          (key_matrix): Linear(in_features=64, out_features=64, bias=False)\n          (value_matrix): Linear(in_features=64, out_features=64, bias=False)\n          (out): Linear(in_features=512, out_features=512, bias=True)\n        )\n        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (feed_forward): Sequential(\n          (0): Linear(in_features=512, out_features=2048, bias=True)\n          (1): ReLU()\n          (2): Linear(in_features=2048, out_features=512, bias=True)\n        )\n        (dropout1): Dropout(p=0.2, inplace=False)\n        (dropout2): Dropout(p=0.2, inplace=False)\n      )\n    )\n  )\n  (decoder): TransformerDecoder(\n    (word_embedding): Embedding(11, 512)\n    (position_embedding): PositionalEmbedding()\n    (layers): ModuleList(\n      (0-5): 6 x DecoderBlock(\n        (attention): MultiHeadAttention(\n          (query_matrix): Linear(in_features=64, out_features=64, bias=False)\n          (key_matrix): Linear(in_features=64, out_features=64, bias=False)\n          (value_matrix): Linear(in_features=64, out_features=64, bias=False)\n          (out): Linear(in_features=512, out_features=512, bias=True)\n        )\n        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (dropout): Dropout(p=0.2, inplace=False)\n        (transformer_block): TransformerBlock(\n          (attention): MultiHeadAttention(\n            (query_matrix): Linear(in_features=64, out_features=64, bias=False)\n            (key_matrix): Linear(in_features=64, out_features=64, bias=False)\n            (value_matrix): Linear(in_features=64, out_features=64, bias=False)\n            (out): Linear(in_features=512, out_features=512, bias=True)\n          )\n          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (feed_forward): Sequential(\n            (0): Linear(in_features=512, out_features=2048, bias=True)\n            (1): ReLU()\n            (2): Linear(in_features=2048, out_features=512, bias=True)\n          )\n          (dropout1): Dropout(p=0.2, inplace=False)\n          (dropout2): Dropout(p=0.2, inplace=False)\n        )\n      )\n    )\n    (fc_out): Linear(in_features=512, out_features=11, bias=True)\n    (dropout): Dropout(p=0.2, inplace=False)\n  )\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Performing a forward pass through the model\nsrc = torch.tensor([[0, 2, 5, 6, 4, 3, 9, 5, 2, 9, 10, 1], \n                    [0, 2, 8, 7, 3, 4, 5, 6, 7, 2, 10, 1]])\ntarget = torch.tensor([[0, 1, 7, 4, 3, 5, 9, 2, 8, 10, 9, 1], \n                       [0, 1, 5, 6, 2, 4, 7, 6, 2, 8, 10, 1]])","metadata":{"execution":{"iopub.status.busy":"2024-03-20T12:26:34.214444Z","iopub.execute_input":"2024-03-20T12:26:34.214801Z","iopub.status.idle":"2024-03-20T12:26:34.228205Z","shell.execute_reply.started":"2024-03-20T12:26:34.214769Z","shell.execute_reply":"2024-03-20T12:26:34.227306Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"# Printing the shapes of the input tensors\nprint(\"Source tensor shape:\", src.shape)\nprint(\"Target tensor shape:\", target.shape)","metadata":{"execution":{"iopub.status.busy":"2024-03-20T12:26:34.229297Z","iopub.execute_input":"2024-03-20T12:26:34.229676Z","iopub.status.idle":"2024-03-20T12:26:34.242023Z","shell.execute_reply.started":"2024-03-20T12:26:34.229646Z","shell.execute_reply":"2024-03-20T12:26:34.241012Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"Source tensor shape: torch.Size([2, 12])\nTarget tensor shape: torch.Size([2, 12])\n","output_type":"stream"}]},{"cell_type":"code","source":"# Performing a forward pass through the model\nout = model(src, target)\n\n# Printing the output tensor shape\nprint(\"Output tensor shape:\", out.shape)","metadata":{"execution":{"iopub.status.busy":"2024-03-20T12:26:34.243105Z","iopub.execute_input":"2024-03-20T12:26:34.243416Z","iopub.status.idle":"2024-03-20T12:26:34.403750Z","shell.execute_reply.started":"2024-03-20T12:26:34.243384Z","shell.execute_reply":"2024-03-20T12:26:34.402856Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"Output tensor shape: torch.Size([2, 12, 11])\n","output_type":"stream"}]},{"cell_type":"code","source":"# Inference\nmodel = Transformer(embed_dim=512, src_vocab_size=src_vocab_size, \n                    target_vocab_size=target_vocab_size, seq_length=seq_length, \n                    num_layers=num_layers, expansion_factor=4, n_heads=8)\n\n# Defining source and target tensors for inference\nsrc_inference = torch.tensor([[0, 2, 5, 6, 4, 3, 9, 5, 2, 9, 10, 1]])\ntrg_inference = torch.tensor([[0]])\n\n# Performing inference\noutput_labels = model.decode(src_inference, trg_inference)\n\n# Printing the output labels\nprint(\"Inference output labels:\", output_labels)","metadata":{"execution":{"iopub.status.busy":"2024-03-20T12:26:34.404863Z","iopub.execute_input":"2024-03-20T12:26:34.405145Z","iopub.status.idle":"2024-03-20T12:26:35.077546Z","shell.execute_reply.started":"2024-03-20T12:26:34.405121Z","shell.execute_reply":"2024-03-20T12:26:35.076531Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"Inference output labels: [2, 2, 2, 4, 2, 8, 6, 6, 6, 2, 2, 8]\n","output_type":"stream"}]},{"cell_type":"code","source":"actual_words = []\npredicted_words = []\nfor actual_label, predicted_label in zip(target[1], output_labels):\n    actual_label = actual_label.item()\n    \n    if actual_label in reverse_target_char_index:\n        actual_word = reverse_target_char_index[actual_label]\n    else:\n        actual_word = '<unknown>'  \n    actual_words.append(actual_word)\n    \n    # Convert predicted label to word if it exists in the dictionary, otherwise mark it as unknown\n    if predicted_label in reverse_target_char_index:\n        predicted_word = reverse_target_char_index[predicted_label]\n    else:\n        predicted_word = '<unknow>'  \n    predicted_words.append(predicted_word)\n\nactual_sentence = ' '.join(actual_words[1:-1]) \npredicted_sentence = ' '.join(predicted_words[1:-1])  \n\nprint(\"Actual Sentence:\", actual_sentence)\nprint(\"Predicted Sentence:\", predicted_sentence)","metadata":{"execution":{"iopub.status.busy":"2024-03-20T12:29:36.918525Z","iopub.execute_input":"2024-03-20T12:29:36.918873Z","iopub.status.idle":"2024-03-20T12:29:36.927279Z","shell.execute_reply.started":"2024-03-20T12:29:36.918845Z","shell.execute_reply":"2024-03-20T12:29:36.926351Z"},"trusted":true},"execution_count":56,"outputs":[{"name":"stdout","text":"Actual Sentence: a ackermann actuated ab accelerometer africa actuated ab aids alain\nPredicted Sentence: ab ab accelerometer ab aids actuated actuated actuated ab ab\n","output_type":"stream"}]},{"cell_type":"code","source":"import nltk\nfrom nltk.translate.bleu_score import sentence_bleu\n\nreference = [actual_sentence.split()]\npredicted = predicted_sentence.split()\n\nbleu_score = sentence_bleu(reference, predicted)\n\nprint(\"BLEU Score:\", bleu_score)","metadata":{"execution":{"iopub.status.busy":"2024-03-20T12:29:37.880200Z","iopub.execute_input":"2024-03-20T12:29:37.881044Z","iopub.status.idle":"2024-03-20T12:29:37.886958Z","shell.execute_reply.started":"2024-03-20T12:29:37.881013Z","shell.execute_reply":"2024-03-20T12:29:37.885988Z"},"trusted":true},"execution_count":57,"outputs":[{"name":"stdout","text":"BLEU Score: 0.668740304976422\n","output_type":"stream"}]}]}